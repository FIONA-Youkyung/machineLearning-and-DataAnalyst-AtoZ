{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. 키워드를 활용한 텍스트 마이닝\n",
    "## 텍스트 마이닝 심화\n",
    "\n",
    "텍스트 마이님 - 머신러닝 기법 : LDA(토픽 모델링 기법), SVM(문서 분류 기법) 등\n",
    "                    - 딥러닝 기법 : RNN, SLTM, TRansformer, BERT 등\n",
    "                    \n",
    "                    워드 임베딩은 딥러닝 기법들의 입력값으로 사용된다. \n",
    "                    그럼 워드 임베딩은 무엇인가?\n",
    "                    \n",
    "## 워드 임베딩(word + Embedding) : *단어*를 컴퓨터가 이해할 수 있는 *벡터*로 표현하는 방법\n",
    "(어떤 단어가 어떤 공간에 표현되는구나 생각하면 됨) \n",
    "\n",
    "#### 워드 임베딩의 대표적인 방법\n",
    "    - Sparse Representation: BOW, TF-IDF\n",
    "        - Sparse Representation의 문제점 : \n",
    "            - 문서 데이터에 존재하는 모든 유니크한 단어수가 벡터의 차원이 되어 고차원 공간이 됨 그러나 그 중 대부분은 0이다. \n",
    "            - 단어의 문맥 정보가 사라짐 (ex : 문장 내 순서(word order ex) New York Times), 문장 내 동시등장(co-occurrence)ex) apple tree / apple iphone)\n",
    "            - 차원의 저주로 인해 분석기법의 성능이 악화됨 , 데이터 공간이 너무 커서 data overfittingm \n",
    "    - Dense Representation : word2vec, Glove 등\n",
    "        - Sparse로 인해 생긴 문제점으로 인해 등장한 Dense Representation의 중요성\n",
    "            - 이미지나 오디오 데이터는 양질의 고차원 데이터로 표현된다. (dense representation) 그런데 text data는 sparse하여 이로 인한 문제가 야기된다. \n",
    "            - Sparse 한 vector space model은 단어를 discrete symbol 로 표시하기 때문에 정보 전달력이 떨어짐\n",
    "            - predictive model을 사용하여 단어의 주변 정보를 반영한 dense representation을 표현함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워드 임베딩 역사 :  NPLM -> word2vec -> fastText -> ELMo\n",
    "\n",
    "1. NPLM(neural probabilistic Language Model)\n",
    "    - 처음으로 제안된 dense representation model\n",
    "    - distributional representation : 어떤 하나의 벡터 위치가 하나의 단어를 표현하는 것이 아니라 여러 위치의 정보로 녹아있다 라는 말..( 앞뒤 문맥을 이용해서 앞뒤의 단어들로부터 학습을 하겠다. 라는 모델을 처음으로 제안\n",
    "    - Neural Network를 이용하여 주변 단어의 등장 확률을 예측함\n",
    " \n",
    "2. word2vec\n",
    "    - Skip-Gram with Negative Sampling : word2vec가 제안한 기법 중 성능이 좋은 것 \n",
    "    - 높은 계산량을 요구하는 문제점을 획기적으로 해결 (어떻게? 는  뒤에서 정리)\n",
    "    - 본격적인 word embedding 시대 개막\n",
    "    \n",
    "3. fastText \n",
    "    - Subword SGNS\n",
    "    - word2vec의 가장 큰 단점 : OOV(out-of-Vocabulary) 단어 기준으로 벡터가 나오는데 학습에 없는 단어(ex) 신조어) 는 알지 못한다. \n",
    "    - 그래서 학습 단위를 subword(char N-gram)로 변경. \n",
    "    - ex) <queen>, N = 3 은, <qu, que, uee, een, en> 로 쪼개어서 학습하겠다. \n",
    "    - 한글에서 많이 쓰인다.(초/중/종성)\n",
    "    \n",
    "4. ELMo(Embedding from Language Model)\n",
    "    - Pretained Language Model을 제안 : 언어를 이해하는 모델\n",
    "    - Bi-diretional Language Model을 제안하여 문맥을 반영하 워드 임베딩 기법 제시 실제 단어의 위치등도 학습하여 , 동음의이어 등의 학습이 가능해짐. \n",
    "    - NLP에서 transfer learning이 확산됨 \n",
    "    \n",
    "    **논문 읽어보면 재미있겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 워드 임베딩 공간의 특징\n",
    "\n",
    "1. 단어 간의 관계 정의 : vector연산으로 단어 간의 관계를 파악할 수 있다. \n",
    "ex) verb tense, Country-Capital\n",
    "2. 비슷한 의미를 가지는 단어들(상대적으로 가깝게 위치하는) 이 군집화 형성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
