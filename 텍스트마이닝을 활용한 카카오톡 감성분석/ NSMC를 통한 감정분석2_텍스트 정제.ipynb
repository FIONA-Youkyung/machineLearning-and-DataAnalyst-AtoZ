{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSMC를 통한 감정분석\n",
    "## nsmc 불러와서 텍스트 정제하기\n",
    ": 감정 분석을 위해 Naver Movie Corpus( https://github.com/e9t/nsmc)을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. nsmc 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(filename):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        documents = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        documents = documents[1:]\n",
    "        \n",
    "    return documents\n",
    "    \n",
    "train_docs = read_documents(\"/Users/jeonghyeonjeong/for github/머신러닝_데이터분석A-Z_패스트캠퍼스/텍스트마이닝을 활용한 카카오톡 감성분석/data/nsmc/ratings_train.txt\")\n",
    "test_docs = read_documents(\"/Users/jeonghyeonjeong/for github/머신러닝_데이터분석A-Z_패스트캠퍼스/텍스트마이닝을 활용한 카카오톡 감성분석/data/nsmc/ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_docs)) #15만 개\n",
    "print(len(test_docs)) #5만 개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 함수 정의 - 저번에 한 것 그대로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(doc):\n",
    "    # 한국어를 제외한 글자를 제거하는 함수.\n",
    "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "    return doc\n",
    "\n",
    "def define_stopwords(path):\n",
    "    \n",
    "    SW = set()\n",
    "    # 불용어를 추가하는 방법 1.\n",
    "    # SW.add(\"있다\")\n",
    "    \n",
    "    # 불용어를 추가하는 방법 2.\n",
    "    # stopwords-ko.txt에 직접 추가\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "            \n",
    "    return SW\n",
    "\n",
    "def text_tokenizing(doc):\n",
    "    return [word for word in mecab.morphs(doc) if word not in SW and len(word) > 1]\n",
    "    \n",
    "    # wordcloud를 위해 명사만 추출하는 경우.\n",
    "    #return [word for word in mecab.nouns(doc) if word not in SW and len(word) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 불러온 데이터를 품사 태그를 붙여서 토크나이징 한다. \n",
    ": 영화리뷰는 앞서 했던 네이버 기사 분석의 기사 보다 더 sns틱하다. 'ㅋㅋㅋㅋㅋ'이렇게 완벅한 한국어가 아닌 것도 있고,, 그래서 형태소 분석기를 사용해서 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['진짜', '짜증', '네요', '목소리'], '0')\n",
      "(['GDNTOPCLASSINTHECLUB'], '0')\n"
     ]
    }
   ],
   "source": [
    "#이 파일이 있으면 불러오고, 없으면 해야된다는 코드짜기\n",
    "#텍스트가 총 20만개 이기 때문에, 정제하는데 시간이 걸린다. \n",
    "#한 번 만들고 나서는 코드를 다시 실행시켰을 때, 또 정제할 필요 없이 불러오게 코드를 짜려 한다.  -- 아 여기 이 부분 무슨 말인지 모르겠음..--\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Okt\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "#형태소 분석기 불러오기\n",
    "okt = Okt()\n",
    "mecab = Mecab()\n",
    "\n",
    "SW = define_stopwords(\"/Users/jeonghyeonjeong/for github/머신러닝_데이터분석A-Z_패스트캠퍼스/텍스트마이닝을 활용한 카카오톡 감성분석/data/stopwords-ko.txt\")\n",
    "\n",
    "if os.path.exists('train_docs.json'):\n",
    "    with open(\"train_docs.json\", encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "else:\n",
    "    train_data = [(text_tokenizing(line[1]), line[2]) for line in train_docs if text_tokenizing(line[1])]\n",
    "#     train_data = []\n",
    "#     for line in train_docs:\n",
    "#         if text_tokenizing(line[1]):\n",
    "#             train_data.append((text_tokenzing(line[1]), line[2]))\n",
    "    \n",
    "    \n",
    "    with open(\"train_docs.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False, indent='\\t')\n",
    "        \n",
    "if os.path.exists('test_docs.json'):\n",
    "    with open(\"test_docs.json\", encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "else:\n",
    "    test_data = [(text_tokenizing(line[1]), line[2]) for line in test_docs if text_tokenizing(line[1])]\n",
    "    \n",
    "    with open(\"test_docs.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_data, f, ensure_ascii=False, indent='\\t')\n",
    "\n",
    "pprint(train_data[0])\n",
    "pprint(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
